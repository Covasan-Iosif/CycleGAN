# Benchmark Performance

My solution for the "Iâ€™m Something of a Painter Myself" competition involves a Cycle-Consistent Adversarial Network model inspired by the scientific [paper](https://arxiv.org/abs/1703.10593) "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks" which I have enhanced and adapted to meet the competition's requirements and limitations. In contrast to the model in the original paper, my model employs a U-net architecture for both generators. Given the relatively small size of the dataset, consisting of only 300 Monet paintings, U-net can capitalize on its ability to learn finer feature relationships, having a higher number of feedforward and backward connections. Due to time constraints, with the model having only 5 hours for training, U-net's simpler workflow provides better results. <br> Moreover, image augmentation has been employed during training through scaling, horizontal flipping, and color variation transformations, enhancing the learning process by introducing increased diversity into the images. <br> Furthermore, I attempted to stabilize the model training procedure by replacing the negative log likelihood objective with a least-squares loss in the adversarial loss formula, as suggested in the original paper. <br>
The best result achieved, based on the MiFID score automatically calculated by Kaggle, is 41.45087, while training on a GPU. This marks a substantial improvement compared to the baseline model provided by the competition, which attained a score of 53.76998, while training on a more powerful TPU. My best result secures a 14th position on the competition's public leaderboard, while the best score is 34.43192.  <br> 
Link to the Kaggle notebook used for submitting to the competition: [notebook](https://www.kaggle.com/code/iosifcovasan/photo-to-monet-using-cyclegan) Version 10
<br> <br>
![Leaderboard](https://github.com/Covasan-Iosif/CycleGAN/blob/main/leaderboard.jpg)
